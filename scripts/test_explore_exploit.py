#!/usr/bin/env python3
"""
Test Gen21 explore/exploit capabilities with 8/2 ratio seed.

This script tests whether Gen21's architecture supports the explore/exploit
paradigm and validates the 8/2 ratio specification.

Tests:
1. Quality Diversity (QD) architecture presence
2. Mutation/variation mechanisms
3. Portfolio diversity support
4. Risk management for exploration
5. Metrics for exploration vs exploitation
"""

from __future__ import annotations
import re
import json
import sys
import random
from pathlib import Path
from typing import List, Dict, Any, Tuple


REPO_ROOT = Path(__file__).resolve().parents[1]
GEN21_PATH = REPO_ROOT / "hfo_gem" / "gen_21" / "gpt5-attempt-3-gem.md"


class ExploreExploitTester:
    """Test explore/exploit capabilities with 8/2 seed."""
    
    def __init__(self, seed: int = 42):
        self.seed = seed
        random.seed(seed)
        self.content = ""
        self.test_results: List[Dict[str, Any]] = []
        
    def load_gen21(self) -> None:
        """Load Gen21 document."""
        if not GEN21_PATH.exists():
            self.add_result("load", 0.0, "Gen21 not found")
            return
        
        self.content = GEN21_PATH.read_text(encoding="utf-8")
        self.add_result("load", 1.0, f"Loaded {len(self.content)} chars")
    
    def test_qd_architecture(self) -> None:
        """Test Quality Diversity architecture presence."""
        qd_patterns = [
            r'quality.*diversity',
            r'\bQD\b',
            r'diverse.*portfolio',
            r'variant',
            r'monoculture',
        ]
        
        found_patterns = []
        for pattern in qd_patterns:
            matches = re.findall(pattern, self.content, re.I)
            if matches:
                found_patterns.append((pattern, len(matches)))
        
        # Score based on how many patterns found
        score = min(1.0, len(found_patterns) / len(qd_patterns))
        
        self.add_result("qd_architecture", score,
                       f"Found {len(found_patterns)}/{len(qd_patterns)} QD patterns")
    
    def test_mutation_mechanisms(self) -> None:
        """Test mutation/variation mechanisms."""
        mutation_patterns = [
            r'\bmutate\b',
            r'\bmutation\b',
            r'variation',
            r'evolve',
            r'evolutionary',
        ]
        
        found = sum(1 for pattern in mutation_patterns 
                   if re.search(pattern, self.content, re.I))
        
        score = min(1.0, found / len(mutation_patterns))
        
        self.add_result("mutation_mechanisms", score,
                       f"Found {found}/{len(mutation_patterns)} mutation patterns")
    
    def test_portfolio_diversity(self) -> None:
        """Test portfolio diversity support."""
        # Look for multi-path or fallback mechanisms
        diversity_indicators = [
            r'at least two paths',
            r'primary.*fallback',
            r'alternative',
            r'backup',
            r'redundant',
        ]
        
        found = sum(1 for pattern in diversity_indicators 
                   if re.search(pattern, self.content, re.I))
        
        score = min(1.0, found / 3)  # Need at least 3 indicators
        
        self.add_result("portfolio_diversity", score,
                       f"Found {found} diversity indicators")
    
    def test_exploration_risk_management(self) -> None:
        """Test risk management for exploration."""
        # Look for risk management in context of exploration
        risk_patterns = [
            r'risk.*register',
            r'risk.*heatmap',
            r'mitigation',
            r'spike.*validate',
            r'canary',
        ]
        
        found = sum(1 for pattern in risk_patterns 
                   if re.search(pattern, self.content, re.I))
        
        score = min(1.0, found / len(risk_patterns))
        
        self.add_result("exploration_risk", score,
                       f"Found {found}/{len(risk_patterns)} risk patterns")
    
    def test_eight_two_ratio(self) -> None:
        """Test for 8/2 or 80/20 ratio specification."""
        ratio_patterns = [
            r'8[/:]2',
            r'80[/:]20',
            r'pareto',
            r'80.*20',
        ]
        
        found = []
        for pattern in ratio_patterns:
            if re.search(pattern, self.content, re.I):
                found.append(pattern)
        
        score = 1.0 if found else 0.0
        
        self.add_result("eight_two_ratio", score,
                       f"8/2 ratio {'found' if found else 'not found'}: {found}")
    
    def test_exploration_metrics(self) -> None:
        """Test for metrics supporting exploration."""
        metrics_patterns = [
            r'metric',
            r'measure',
            r'fitness',
            r'score',
            r'evaluate',
        ]
        
        found = sum(1 for pattern in metrics_patterns 
                   if re.search(pattern, self.content, re.I))
        
        score = min(1.0, found / len(metrics_patterns))
        
        self.add_result("exploration_metrics", score,
                       f"Found {found}/{len(metrics_patterns)} metric patterns")
    
    def test_swarm_d3a(self) -> None:
        """Test SWARM D3A + Mutate specification."""
        # SWARM should include mutation evolution
        swarm_pattern = r'SWARM.*D3A.*Mutate'
        d3a_steps = ['Decide', 'Detect', 'Deliver', 'Assess', 'Mutate']
        
        has_swarm = bool(re.search(swarm_pattern, self.content, re.DOTALL | re.I))
        found_steps = sum(1 for step in d3a_steps 
                         if step in self.content)
        
        score = (0.5 if has_swarm else 0.0) + (0.5 * found_steps / len(d3a_steps))
        
        self.add_result("swarm_d3a", score,
                       f"SWARM: {has_swarm}, D3A steps: {found_steps}/{len(d3a_steps)}")
    
    def simulate_explore_exploit(self, iterations: int = 100) -> Tuple[int, int]:
        """Simulate explore/exploit decisions with 8/2 ratio."""
        # This is a toy simulation to demonstrate the concept
        explore_count = 0
        exploit_count = 0
        
        for _ in range(iterations):
            # 80% exploit, 20% explore
            if random.random() < 0.8:
                exploit_count += 1
            else:
                explore_count += 1
        
        return explore_count, exploit_count
    
    def test_simulation(self) -> None:
        """Test explore/exploit simulation."""
        explore, exploit = self.simulate_explore_exploit(iterations=1000)
        
        # Check if ratio is approximately 2:8
        total = explore + exploit
        explore_ratio = explore / total
        exploit_ratio = exploit / total
        
        # Allow 5% tolerance
        expected_explore = 0.20
        expected_exploit = 0.80
        
        explore_ok = abs(explore_ratio - expected_explore) < 0.05
        exploit_ok = abs(exploit_ratio - expected_exploit) < 0.05
        
        score = 1.0 if (explore_ok and exploit_ok) else 0.5
        
        self.add_result("simulation", score,
                       f"Explore: {explore_ratio:.2%} (target 20%), " +
                       f"Exploit: {exploit_ratio:.2%} (target 80%)")
    
    def add_result(self, test_name: str, score: float, details: str) -> None:
        """Add a test result."""
        status = "PASS" if score >= 0.7 else "WARN" if score >= 0.4 else "FAIL"
        self.test_results.append({
            "test": test_name,
            "score": score,
            "status": status,
            "details": details
        })
    
    def run_tests(self) -> Dict[str, Any]:
        """Run all explore/exploit tests."""
        self.load_gen21()
        
        if self.content:
            self.test_qd_architecture()
            self.test_mutation_mechanisms()
            self.test_portfolio_diversity()
            self.test_exploration_risk_management()
            self.test_eight_two_ratio()
            self.test_exploration_metrics()
            self.test_swarm_d3a()
            self.test_simulation()
        
        total_score = sum(r["score"] for r in self.test_results)
        max_score = len(self.test_results)
        avg_score = total_score / max_score if max_score > 0 else 0.0
        
        passed = len([r for r in self.test_results if r["status"] == "PASS"])
        warned = len([r for r in self.test_results if r["status"] == "WARN"])
        failed = len([r for r in self.test_results if r["status"] == "FAIL"])
        
        return {
            "seed": self.seed,
            "total_tests": len(self.test_results),
            "passed": passed,
            "warned": warned,
            "failed": failed,
            "average_score": avg_score,
            "explore_exploit_support": avg_score >= 0.7,
            "test_results": self.test_results
        }


def main() -> int:
    """Run explore/exploit tests with 8/2 seed."""
    # Use seed for reproducibility
    seed = 42
    if len(sys.argv) > 1:
        seed = int(sys.argv[1])
    
    tester = ExploreExploitTester(seed=seed)
    results = tester.run_tests()
    
    print(json.dumps(results, indent=2))
    
    return 0 if results["explore_exploit_support"] else 1


if __name__ == "__main__":
    sys.exit(main())
